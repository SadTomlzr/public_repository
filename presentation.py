# -*- coding: utf-8 -*-
"""presentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qwMeKJH89lnn2rmYHF9vP63sbf0RJL_2

# 202210xx 组会 NLP预训练与下游任务代码实现
  文档主要内容是我在入门nlp时所遇到的一些代码上的问题，积累总结下来成为文档。主要解决的是：

  1. 训练自己的数据集
  2. 在深度学习看源码的时候常常看到有很多util和包，让我不能理解。在查阅资料后得知在dl实验过程中基本需要三个部分，分别为dataloader、trainer、model。其中model根据不同论文中的实现不一样内容也大不一样。但是dataloader和trainer大多都是一个流程。所以在第一部分简单的进行了实现，也理解了整个训练的流程。
  3. 一开始对于单词为什么要预训练和得到的模型可以找到单词的相似单词这个结论只是看书觉得有些抽象，不知道数据的内容和形状。所以我也进行了简单的实现，本文只实现了word2vector和bert两个方式的预训练。
  4. 下游任务的实现（但是只是简单的demo）

  当前文档主要包含了以下内容：
  - lstm实现情感分析
   - 基础案例
   - dataloader
   - trainer
  - 预训练（word2vector和bert预训练实现）
   - word2vector实现
   - bert实现
  - nlp下游任务实现
   - glove微调lstm实现情感分析
   - bert微调实现语义推断
"""

# 让当前笔记本挂载Google云硬盘
# from google.colab import drive
# drive.mount('/content/drive')

# 如果不在colab实现，则不运行当前单元格
!cp drive/MyDrive/train.csv train.csv

"""## lstm实现情感分析

### 基础案例
"""

# 导包
import torch
import torch.nn as nn
import torch.optim as optim
import collections
import torch.utils.data as Data
from torch.autograd import Variable
import jieba
import pandas as pd
import numpy as np
!pip install d2l==0.17.1
!pip install matplotlib==3.0.0
from d2l import torch as d2l

# 数据
from google.colab import data_table
from vega_datasets import data
# colab 插件 开启 datatable
data_table.enable_dataframe_formatter()

df = pd.read_csv("train.csv")
df = df.sample(frac=1)
df[:10]

# 转为数组
seq = df["review"].tolist()
label = df["label"].tolist()
display(seq[:3])
display(label[:3])

#分词
seq_cut = []
seq_cut_list = []
for i in seq:
    cut_res = list(jieba.cut(i))
    # 收集每次出现的单词
    seq_cut = seq_cut + cut_res
    # 收集每次出现的句子
    seq_cut_list.append(cut_res)
word2num = sorted(collections.Counter(seq_cut).items(), key=lambda item: item[1], reverse=True)
# 所有词
vocab = list(set(seq_cut))
# 词对应索引
word2index = {w[0]: i+1 for i, w in enumerate(word2num)}
word2index["PAD"] = 0

# 词典大小
vocab_size = len(word2index)
# 句子最大长度
seq_length = max([len(i) for i in seq_cut_list])
embed_size = 3
num_classes = 2
num_hiddens = 5

def prepare_data(seq,label):
    inputs = []
    for i in seq:
        seq_index = [word2index[word] for word in i]
        # 句子长度补齐
        if len(seq_index)!=seq_length:
            seq_index = seq_index + [0] * (seq_length-len(seq_index))
        inputs.append(seq_index)
    targets = [i for i in label]
    return inputs, targets
input_batch_old,target_batch_old = prepare_data(seq_cut_list,label)
input_batch,target_batch = Variable(torch.LongTensor(input_batch_old)),Variable(torch.LongTensor(target_batch_old))

def try_gpu(i=0):
    """Return gpu(i) if exists, otherwise return cpu()."""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
device = try_gpu()
device

class BiLSTM(nn.Module):
    def __init__(self, **kwargs):
        super(BiLSTM, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # 将bidirectional设置为True以获取双向循环神经⽹络
        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=2,bidirectional=True)
        self.decoder = nn.Linear(4 * num_hiddens, 2)
    def forward(self, inputs):
        # inputs的形状是（批量⼤⼩，时间步数）
        # 因为⻓短期记忆⽹络要求其输⼊的第⼀个维度是时间维，
        # 所以在获得词元表⽰之前，输⼊会被转置。
        # 输出形状为（时间步数，批量⼤⼩，词向量维度）
        embeddings = self.embedding(inputs.T)
        self.encoder.flatten_parameters()
        # 返回上⼀个隐藏层在不同时间步的隐状态，
        # outputs的形状是（时间步数，批量⼤⼩，2*隐藏单元数）
        outputs, _ = self.encoder(embeddings)
        # 连结初始和最终时间步的隐状态，作为全连接层的输⼊，
        # 其形状为（批量⼤⼩，4*隐藏单元数）
        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)
        outs = self.decoder(encoding)
        return outs

net = BiLSTM()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)
X = input_batch.to(device)
net.to(device)
Y = target_batch.to(device)

# 训练
for epoch in range(1000):
    net.train()
    pred = net(X)
    loss = criterion(pred, Y)
    if (epoch+1)%200==0:
      print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

"""### dataloader&trainer构建

#### dataloader
"""

import random

def seq_data_iter_sequential(train, label, batch_size, num_steps=1):
    """使⽤顺序分区⽣成⼀个⼩批量⼦序列"""
    # 从随机偏移量开始划分序列
    item_len = len(train[0])

    num_tokens = (len(train) // batch_size) * batch_size
    Xs = torch.tensor(train[:num_tokens])
    Ys = torch.tensor(label[:num_tokens])

    Xs, Ys = Xs.reshape(-1, batch_size, item_len), Ys.reshape(-1,batch_size)
    num_batches = Xs.shape[0] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[i]
        Y = Ys[i]
        yield X, Y

# seq_data_iter_sequential 使用demo
test_train_seq = []
for i in range(35):
  test_train_seq.append(list(range(i*20,(i+1)*20)))

test_label_seq = list(range(0,35))
for i,j in seq_data_iter_sequential(test_train_seq,test_label_seq,3):
  print("x:",i,"\ny:",j)
  break

# loader类
class DataLoader:
    """An iterator to load sequence data."""
    def __init__(self, batch_size, num_steps=1):
        self.data_iter_fn = seq_data_iter_sequential
        self.train_data, self.train_label = input_batch_old, target_batch_old
        self.batch_size = batch_size
        self.num_batches = len(self.train_data) // batch_size

    def __iter__(self):
        return self.data_iter_fn(self.train_data, self.train_label, self.batch_size)

# load方法
def load_data(batch_size):
  """返回数据集的迭代器"""
  data_iter = DataLoader(batch_size)
  return data_iter

"""#### trainer"""

batch_size = 64
train_iter = load_data(batch_size)
for x,y in train_iter:
  print("每个batch数据的内容")
  display("X:",x,"Y:",y)
  break

def accuracy(y_hat, y):
  """Compute the number of correct predictions."""
  if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
      y_hat = argmax(y_hat, axis=1)
  cmp = astype(y_hat, y.dtype) == y
  return float(reduce_sum(astype(cmp, y.dtype)))

def train_one_batch(net,x,y,loss,trainer,device):
  x = x.to(device)
  y = y.to(device)

  trainer.zero_grad()
  pred = net(x)
  l = loss(pred, y)
  l.sum().backward()
  trainer.step()
  train_loss_sum = l.sum()
  train_acc_sum = accuracy(pred, y)
  return train_loss_sum, train_acc_sum

argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)
astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)
reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)

def train(net, train_iter, test_iter, loss, trainer, num_epochs,device):
    """Train a model with GPU"""
    num_batches = train_iter.num_batches
    net.to(device)
    net.train()

    for epoch in range(num_epochs):
        # Sum of training loss, sum of training accuracy, no. of examples,
        # no. of predictions
        for i, (features, labels) in enumerate(train_iter):
            l, acc = train_one_batch(net, features, labels, loss, trainer,device)
            if (i + 1) % (num_batches) == 0 or i == num_batches - 1:
                print("epoch:",epoch,"acc:",acc,"loss",l)
    # print(f'loss {metric[0] / metric[2]:.3f}, train acc '
    #       f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
    # print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
    #       f'{str(devices)}')

lr, num_epochs = 0.01, 20
net = BiLSTM()
optimizer = optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction="none")
train(net,train_iter,0,loss,optimizer,num_epochs,device)

"""## 预训练（word2vector和bert）

### word2vector
skip-gram
<div align=center> 
<img src="https://aweidao1.com/image/thumb/2022-09-22/1663852754249744PKvhqY9L.png" width="60%"/>
</div>

cbow
<div align=center> 
<img src="https://aweidao1.com/image/thumb/2022-09-22/1663852776354795QV04IncU.png" width="60%"/>
</div>

任务可视化

可以知道的是在skip-gram里，我们需要的是将center词做输入，预测上下文词，是一个多分类任务，类别数是词典的上下文大小。
<div align = center>
<img src="https://aweidao1.com/image/thumb/2022-09-22/1663853672009861TxJ5WEC2.png"/>
</div>

cbow同理，上下文预测中心词，分类数为词典的大小。
"""

import math
import os
import random
import torch
from d2l import torch as d2l

d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')

def read_ptb():
  """将PTB数据集加载到⽂本⾏的列表中"""
  data_dir = d2l.download_extract('ptb')
  # Readthetrainingset.
  with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
    raw_text = f.read()
  return [line.split() for line in raw_text.split('\n')]
sentences = read_ptb()
f'# sentences数: {len(sentences)}'

vocab = d2l.Vocab(sentences, min_freq=10)
f'vocab size: {len(vocab)}'

"""#### 下采样
针对没有太多意义的高频词汇使用下采样降低出现次数，公式如下

$$ P(w_i) = max(1-\sqrt{t\over f(w_i)},0). $$

&emsp;&emsp;&emsp;$f(w_i)$是$w_i$的出现词数与数据集中的总词数的⽐率</br>&emsp;&emsp;&emsp;常量t是超参数（在实验中为$10^{−4}$）
"""

def subsample(sentences, vocab):
  """下采样⾼频词"""
  # 排除未知词元'<unk>'
  sentences = [[token for token in line if vocab[token] != vocab.unk] for line in sentences]
  # 计算次元出现次数 type:dict
  counter = d2l.count_corpus(sentences)
  num_tokens = sum(counter.values())
  # 如果在下采样期间保留词元，则返回True
  def keep(token):
    # 下采样公式
    # return Boolean 
    # 判断是否保留
    return (random.uniform(0, 1) < math.sqrt(1e-4 / counter[token] * num_tokens))
  return ([[token for token in line if keep(token)] for line in sentences],counter)
subsampled, counter = subsample(sentences, vocab)

# 绘制列表长度对的直方图
d2l.show_list_len_pair_hist(['origin', 'subsampled'], 
       'tokens per sentence','count', sentences, subsampled);
# 图例x轴代表每个句子长度是多少个单词
# 图例y轴代表当前句子长度的个数
# origin代表原来句子组中含有的句子呈正态分布。有很多很长的句子
# subsampled代表下采样后大多数句子只有几个单词。说明下采样去掉了很多单词
# 并且通过公式，可以知道下采样去掉的大都是高频单词。低频单词去掉的概率很低。

def get_centers_and_contexts(corpus, max_window_size):
  """返回跳元模型中的中⼼词和上下⽂词"""
  centers, contexts = [], []
  for line in corpus:
    # 要形成“中⼼词-上下⽂词”对，每个句⼦⾄少需要有2个词
    if len(line) < 2:
      continue
    centers += line
    for i in range(len(line)): # 上下⽂窗⼝中间i
      # 上下文窗口是随机的
      window_size = random.randint(1, max_window_size)
      indices = list(range(max(0, i - window_size),min(len(line), i + 1 + window_size)))
      # 从上下⽂词中排除中⼼词
      indices.remove(i)
      contexts.append([line[idx] for idx in indices])
  return centers, contexts

tiny_dataset = [list(range(7)), list(range(7, 10))]
print('数据集', tiny_dataset)
for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):
  print('中⼼词', center, '的上下⽂词是', context)

# 将词元映射到它们在语料库中的索引
corpus = [vocab[line] for line in subsampled]
display(corpus[:3])
subsampled[:3]

all_centers, all_contexts = get_centers_and_contexts(corpus, 5)
f'# “中⼼词-上下⽂词对”的数量: {sum([len(contexts) for contexts in all_contexts])}'

"""#### 负采样
如果按照skip-gram模型进行任务训练。那么input是 中心词 target是 对应词对：

  例： the man loves his son
  
  input：loves 
  
  target：[the,man,his,son]

> 任务类型为多分类任务，但是中心词很多，并且需要判断对应的target更多，有可能达到几万甚至更多，则sofmmax极难完成当前任务。

&emsp;&emsp;所以这里word2vector转换了任务目标，使用负采样获取负样本（即如例子中[the,man,his,son]以外的单词）。并且改变任务目标——判断 输入是否为loves的上下文词。即

&emsp;&emsp;input1：loves 

&emsp;&emsp;input2：[the,man,his,son,***no,and,ok***] 

&emsp;&emsp;label：[1,1,1,1,***0,0,0***]

【注：加粗则是负样本】

&emsp;&emsp;这里word2vector转换任务目标为二分类任务。任务计算难度大大降低。

&emsp;&emsp;其中还有一些解决办法，比如hierarchical softmax。
"""

# 随机索引生成
class RandomGenerator:
  """
  根据n个采样权重在{1,...,n}中随机抽取
  根据输入的sampling_weights来随机的生成数据 这些数据从population中选取
  population的长度是sampling_weights的长度
  """
  def __init__(self, sampling_weights):
    # 生成一个index 1，2，3，4，5，6...
    self.population = list(range(1, len(sampling_weights) + 1))
    # 采样权重
    self.sampling_weights = sampling_weights
    # 采样候选数组
    self.candidates = []
    self.i = 0
  def draw(self):
    if self.i == len(self.candidates):
      # 缓存k个随机采样结果 这里就在生成需要的index
      self.candidates = random.choices(self.population, self.sampling_weights, k=10000)
      self.i = 0
    self.i += 1
    return self.candidates[self.i - 1]

generator = RandomGenerator([2, 3, 4])
[generator.draw() for _ in range(10)]

# 负采样函数
def get_negatives(all_contexts, vocab, counter, K):
  """
  返回负采样中的噪声词
  all_contexts：所有的上下文
  vocab：词表
  counter：词频统计的dict
  k：每个中心词需要多少负采样词
  """
  # 索引为1、2、...（索引0是词表中排除的未知标记）
  # 根据单词出现频率的0.75次方进行权重计算
  sampling_weights = [counter[vocab.to_tokens(i)]**0.75
                    for i in range(1, len(vocab))]
  all_negatives = []
  # 输入权重list就可以得到index生成器generator
  generator = RandomGenerator(sampling_weights)

  for contexts in all_contexts:
    negatives = []
    while len(negatives) < len(contexts) * K:

      neg = generator.draw()
      # 噪声词不能是上下⽂词
      if neg not in contexts:
        negatives.append(neg)
    all_negatives.append(negatives)
  return all_negatives
all_negatives = get_negatives(all_contexts, vocab, counter, 5)

def batchify(data):
  """返回带有负采样的跳元模型的⼩批量样本"""
  max_len = max(len(c) + len(n) for _, c, n in data)
  centers, contexts_negatives, masks, labels = [], [], [], []
  for center, context, negative in data:
    cur_len = len(context) + len(negative)
    centers += [center]
    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]
    masks += [[1] * cur_len + [0] * (max_len - cur_len)]
    labels += [[1] * len(context) + [0] * (max_len - len(context))]
    
  return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(
  contexts_negatives), torch.tensor(masks), torch.tensor(labels))

# 综合前文的func
def load_data_ptb(batch_size, max_window_size, num_noise_words):
  """下载PTB数据集，然后将其加载到内存中"""
  num_workers = d2l.get_dataloader_workers()
  sentences = read_ptb()
  # 词表
  vocab = d2l.Vocab(sentences, min_freq=10)
  # 下采样和词频统计
  subsampled, counter = subsample(sentences, vocab)
  # 转换的语料
  corpus = [vocab[line] for line in subsampled]
  # 所有的中心词和上下文词
  all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)
  # 所有的负采样词
  all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)

  class PTBDataset(torch.utils.data.Dataset):
    def __init__(self, centers, contexts, negatives):
      assert len(centers) == len(contexts) == len(negatives)
      self.centers = centers
      self.contexts = contexts
      self.negatives = negatives
    def __getitem__(self, index):
      return (self.centers[index], self.contexts[index],self.negatives[index])
    def __len__(self):
      return len(self.centers)

  # 用PTBDataset封装
  dataset = PTBDataset(all_centers, all_contexts, all_negatives)
  # 转换为data_iter
  data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,
                       collate_fn=batchify, num_workers=num_workers)
  return data_iter, vocab

data_iter, vocab = load_data_ptb(5, 5, 5)

names = ['centers', 'contexts_negatives', 'masks', 'labels']
tmp = None
for batch in data_iter:
  for name,data in zip(names,batch):
    print(name,":",data,data.shape)
    if name == "contexts_negatives":
      tmp = data
    # break
  break
#

import math
import torch
from torch import nn
from d2l import torch as d2l
batch_size, max_window_size, num_noise_words = 512, 5, 5
data_iter, vocab = load_data_ptb(batch_size, max_window_size,num_noise_words)

# 前向传播
def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
  """
  num_embeddings: 词表个数
  embedding_dim: 词表维数
  embed = nn.Embedding(num_embeddings=20, embedding_dim=4)
  contexts_and_negatives: 上下文和负采样
  center: 中心词
  """
  v = embed_v(center)
  u = embed_u(contexts_and_negatives)
  # 例：输入shape center:(5,1) contexts_and_negatives:(5,36) 且embed_size = 4
  # 则 v为（5，1，4） u为（5，36，4） 
  # pred为 v与u.permute(0, 2, 1) 矩阵乘法后的结果
  # 其中 u.permute(0, 2, 1)的形状为 （5，4，36）
  pred = torch.bmm(v, u.permute(0, 2, 1))
  return pred

class SigmoidBCELoss(nn.Module):
  # 带掩码的⼆元交叉熵损失
  def __init__(self):
    super().__init__()
  def forward(self, inputs, target, mask=None):
    out = nn.functional.binary_cross_entropy_with_logits(inputs, target, weight=mask, reduction="none")
    return out.mean(dim=1)
loss = SigmoidBCELoss()

embed_size = 50
net = nn.Sequential(
      nn.Embedding(num_embeddings=len(vocab),embedding_dim=embed_size),
      nn.Embedding(num_embeddings=len(vocab),embedding_dim=embed_size)
    )

def train(net, data_iter, lr, num_epochs,loss, device=d2l.try_gpu()):
  def init_weights(m):
    if type(m) == nn.Embedding:
      nn.init.xavier_uniform_(m.weight)
  net.apply(init_weights)

  net = net.to(device)
  optimizer = torch.optim.Adam(net.parameters(), lr=lr)
  animator = d2l.Animator(xlabel='epoch', ylabel='loss',
  xlim=[1, num_epochs])
  # 规范化的损失之和，规范化的损失数
  metric = d2l.Accumulator(2)
  for epoch in range(num_epochs):
    # 计时器和batches
    timer, num_batches = d2l.Timer(), len(data_iter)
    for i, batch in enumerate(data_iter):
      # 常规的训练过程
      optimizer.zero_grad()
      center, context_negative, mask, label = [
      data.to(device) for data in batch]
      
      pred = skip_gram(center, context_negative, net[0], net[1])
      # 因为有掩码，这里是掩码训练loss的实现
      l = (loss(pred.reshape(label.shape).float(), label.float(), mask)/mask.sum(axis=1) * mask.shape[1])
      l.sum().backward()
      optimizer.step()
      # 更新图片
      metric.add(l.sum(), l.numel())
      if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
        animator.add(epoch + (i + 1) / num_batches,(metric[0] / metric[1],))
  print(f'loss {metric[0] / metric[1]:.3f}, 'f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')

lr, num_epochs = 0.002, 5
train(net, data_iter, lr, num_epochs, loss)

def get_similar_tokens(query_token, k, embed):
  W = embed.weight.data
  x = W[vocab[query_token]]
  # 计算余弦相似性。增加1e-9以获得数值稳定性
  cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *
  torch.sum(x * x) + 1e-9)
  topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')
  for i in topk[1:]: # 删除输⼊词
    print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')
get_similar_tokens('chip', 3, net[0])

"""### bert
- bert的输入

1）在序列tokens中把分割token（[SEP]）插入到每个句子后，以分开不同的句子tokens。

2）为每一个token表征都添加一个可学习的分割embedding来指示其属于句子A还是句子B。
<div align=center> 
<img src="https://pic1.zhimg.com/v2-a12ee6f717cc8312c43d140eb173def8_r.jpg"/>
</div>
上面提到了BERT的输入为每一个token对应的表征，实际上该表征是由三部分组成的，分别是对应的token，分割和位置 embeddings
<div align=center> 
<img src="https://pic1.zhimg.com/v2-ee823df66560850baa34128af76a6334_r.jpg"/>
</div>

- BERT构建了两个预训练任务，分别是Masked Language Model和Next Sentence Prediction。

- bert的输出

C为分类token（[CLS]）对应最后一个Transformer的输出， $T_i$则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务（如，序列标注和问答任务），就把 输入到额外的输出层中进行预测。对于一些句子级别的任务（如，自然语言推断和情感分类任务），就把C输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。
<div align=center> 
<img src="https://pic3.zhimg.com/80/v2-f0618dc2c2f62bd8d71c2195947be1d6_720w.jpg"/>
</div>

#### encoder块定义
"""

class BERTEncoder(nn.Module):
  """BERT编码器实现"""
  def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
          ffn_num_hiddens, num_heads, num_layers, dropout,
          max_len=1000, key_size=768, query_size=768, value_size=768,
          **kwargs):
    super(BERTEncoder, self).__init__(**kwargs)
    self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
    self.segment_embedding = nn.Embedding(2, num_hiddens)
    # 在BERT中，位置嵌⼊是可学习的，因此我们创建⼀个⾜够⻓的位置嵌⼊参数
    self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
    num_hiddens))
    # 堆叠
    self.blks = nn.Sequential()
    for i in range(num_layers):
      self.blks.add_module(f"{i}", d2l.EncoderBlock(
      key_size, query_size, value_size, num_hiddens, norm_shape,
      ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
  def forward(self, tokens, segments, valid_lens):
    # 在以下代码段中，X的形状保持不变：（批量⼤⼩，最⼤序列⻓度，num_hiddens）
    # 三个embedding相加
    X = self.token_embedding(tokens) + self.segment_embedding(segments)
    X = X + self.pos_embedding.data[:, :X.shape[1], :]
    # 再在所有block中进行训练
    for blk in self.blks:
      X = blk(X, valid_lens)
    return X

vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,ffn_num_hiddens, num_heads, num_layers, dropout)
"""
将tokens定义为⻓度为8的2个输⼊序列，其中每个词元是词表的索引。使⽤输
⼊tokens的BERTEncoder的前向推断返回编码结果,其中每个词元由向量表⽰，其⻓度由超参
数num_hiddens定义。此超参数通常称为Transformer编码器的隐藏⼤小（隐藏单元数）。
"""
tokens = torch.randint(0, vocab_size, (2, 8))
print(tokens)
segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape

"""#### 子任务定义"""

# 子任务定义
class MaskLM(nn.Module):
  """BERT的掩蔽语⾔模型任务"""
  def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
    super(MaskLM, self).__init__(**kwargs)
    self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
                  nn.ReLU(),
                  nn.LayerNorm(num_hiddens),
                  nn.Linear(num_hiddens, vocab_size))

  def forward(self, X, pred_positions):
    num_pred_positions = pred_positions.shape[1]
    pred_positions = pred_positions.reshape(-1)
    batch_size = X.shape[0]
    # 假设batch_size=2，num_pred_positions=3
    # 那么batch_idx是np.array（[0,1]）
    batch_idx = torch.arange(0, batch_size)
    # repeat_interleave后为np.array（[0,0,0,1,1,1]）
    # 同时也应该为[0,0,0,1,1,1]
    batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)
    masked_X = X[batch_idx, pred_positions]
    
    masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
    mlm_Y_hat = self.mlp(masked_X)
    return mlm_Y_hat

print(encoded_X[[0,0,0,1,1,1],[1, 5, 2, 6, 1, 5]].shape)
encoded_X[[0,0,0,1,1,1],[1, 5, 2, 6, 1, 5]].reshape((2, 3, -1)).shape

"""
我们将mlm_positions定义为在encoded_X的任⼀输⼊序列中预测的3个指⽰。
mlm的前向推断返回encoded_X的所有掩蔽位置mlm_positions处的预测结
果mlm_Y_hat。对于每个预测，结果的⼤小等于词表的⼤小。
"""
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
print(mlm_Y_hat.shape)
"""
通过掩码下的预测词元mlm_Y的真实标签mlm_Y_hat，
我们可以计算在BERT预训练中的遮蔽语⾔模型任务
的交叉熵损失。
"""
mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])
loss = nn.CrossEntropyLoss(reduction='none')
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape

class NextSentencePred(nn.Module):
  """BERT的下⼀句预测任务"""
  def __init__(self, num_inputs, **kwargs):
    super(NextSentencePred, self).__init__(**kwargs)
    self.output = nn.Linear(num_inputs, 2)
  def forward(self, X):
    # X的形状：(batchsize,num_hiddens)
    return self.output(X)

encoded_X.shape[-1]

encoded_X

encoded_X = torch.flatten(encoded_X, start_dim=1)
# NSP的输⼊形状:(batchsize，num_hiddens)
nsp = NextSentencePred(encoded_X.shape[-1])
nsp_Y_hat = nsp(encoded_X)
print(nsp_Y_hat)

nsp_y = torch.tensor([0, 1])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape

"""#### 模型整合"""

class BERTModel(nn.Module):
  """BERT模型"""
  def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
        ffn_num_hiddens, num_heads, num_layers, dropout,
        max_len=1000, key_size=768, query_size=768, value_size=768,
        hid_in_features=768, mlm_in_features=768,
        nsp_in_features=768):
    super(BERTModel, self).__init__()
    self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
                  ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                  dropout, max_len=max_len, key_size=key_size,
                  query_size=query_size, value_size=value_size)
    self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),nn.Tanh())
    self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
    self.nsp = NextSentencePred(nsp_in_features)
  def forward(self, tokens, segments, valid_lens=None,pred_positions=None):
    encoded_X = self.encoder(tokens, segments, valid_lens)
    if pred_positions is not None:
      mlm_Y_hat = self.mlm(encoded_X, pred_positions)
    else:
      mlm_Y_hat = None
    # ⽤于下⼀句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引
    nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
    return encoded_X, mlm_Y_hat, nsp_Y_hat

batch_size, max_len = 512, 64
train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)

net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],
          ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,
          num_layers=2, dropout=0.2, key_size=128, query_size=128,
          value_size=128, hid_in_features=128, mlm_in_features=128,
          nsp_in_features=128)
devices = d2l.try_all_gpus()
loss = nn.CrossEntropyLoss()

"""#### loss计算"""

def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
            segments_X, valid_lens_x,
            pred_positions_X, mlm_weights_X,
            mlm_Y, nsp_y):
  # 前向传播
  _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
                    valid_lens_x.reshape(-1),
                    pred_positions_X)
  # 计算遮蔽语⾔模型损失
  mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\
  mlm_weights_X.reshape(-1, 1)
  mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
  # 计算下⼀句⼦预测任务的损失
  nsp_l = loss(nsp_Y_hat, nsp_y)
  l = mlm_l + nsp_l
  return mlm_l, nsp_l, l

"""#### 训练"""

def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
  net = nn.DataParallel(net, device_ids=devices).to(devices[0])
  trainer = torch.optim.Adam(net.parameters(), lr=0.01)
  step, timer = 0, d2l.Timer()
  animator = d2l.Animator(xlabel='step', ylabel='loss',
  xlim=[1, num_steps], legend=['mlm', 'nsp'])
  # 遮蔽语⾔模型损失的和，下⼀句预测任务损失的和，句⼦对的数量，计数
  metric = d2l.Accumulator(4)
  num_steps_reached = False
  while step < num_steps and not num_steps_reached:
    for tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y in train_iter:
      # 定义变量
      tokens_X = tokens_X.to(devices[0])
      segments_X = segments_X.to(devices[0])
      valid_lens_x = valid_lens_x.to(devices[0])
      pred_positions_X = pred_positions_X.to(devices[0])
      mlm_weights_X = mlm_weights_X.to(devices[0])
      mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])

      # 训练
      trainer.zero_grad()
      # 计时
      timer.start()
      # 计算loss
      mlm_l, nsp_l, l = _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
                            pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
      l.backward()
      trainer.step()
      # 画图
      metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)
      timer.stop()
      animator.add(step + 1,(metric[0] / metric[3], metric[1] / metric[3]))

      step += 1
      if step == num_steps:
        num_steps_reached = True
      break
  print(f'MLM loss {metric[0] / metric[3]:.3f}, '
      f'NSP loss {metric[1] / metric[3]:.3f}')
  print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
      f'{str(devices)}')

train_bert(train_iter, net, loss, len(vocab), devices, 50)

# 用bert表示文本
def get_bert_encoding(net, tokens_a, tokens_b=None):
  tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
  token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)
  segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)
  valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)
  encoded_X, _, _ = net(token_ids, segments, valid_len)
  return encoded_X

tokens_a = ['a', 'crane', 'is', 'flying']
encoded_text = get_bert_encoding(net, tokens_a)
# 词元：'<cls>','a','crane','is','flying','<sep>'
encoded_text_cls = encoded_text[:, 0, :]
encoded_text_crane = encoded_text[:, 2, :]
encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]

tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']
encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)
# 词元：'<cls>','a','crane','driver','came','<sep>','he','just','left','<sep>'
encoded_pair_cls = encoded_pair[:, 0, :]
encoded_pair_crane = encoded_pair[:, 2, :]
encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]

"""## 下游任务实现

### glove微调lstm实现情感分析
"""

import torch
from torch import nn
from d2l import torch as d2l
batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)

class BiRNN(nn.Module):
  def __init__(self, vocab_size, embed_size, num_hiddens,
    num_layers, **kwargs):
    super(BiRNN, self).__init__(**kwargs)
    self.embedding = nn.Embedding(vocab_size, embed_size)
    # 将bidirectional设置为True以获取双向循环神经⽹络
    self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,
    bidirectional=True)
    self.decoder = nn.Linear(4 * num_hiddens, 2)
  def forward(self, inputs):
    # inputs的形状是（批量⼤⼩，时间步数）
    # 因为⻓短期记忆⽹络要求其输⼊的第⼀个维度是时间维，
    # 所以在获得词元表⽰之前，输⼊会被转置。
    # 输出形状为（时间步数，批量⼤⼩，词向量维度）
    embeddings = self.embedding(inputs.T)
    self.encoder.flatten_parameters()
    # 返回上⼀个隐藏层在不同时间步的隐状态，
    # outputs的形状是（时间步数，批量⼤⼩，2*隐藏单元数）
    outputs, _ = self.encoder(embeddings)
    # 连结初始和最终时间步的隐状态，作为全连接层的输⼊，
    # 其形状为（批量⼤⼩，4*隐藏单元数）
    encoding = torch.cat((outputs[0], outputs[-1]), dim=1)
    outs = self.decoder(encoding)
    return outs

embed_size, num_hiddens, num_layers = 100, 100, 2
devices = d2l.try_all_gpus()
net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)

def init_weights(m):
  if type(m) == nn.Linear:
    nn.init.xavier_uniform_(m.weight)
  if type(m) == nn.LSTM:
    for param in m._flat_weights_names:
      if "weight" in param:
        nn.init.xavier_uniform_(m._parameters[param])
net.apply(init_weights);

glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
embeds.shape

net.embedding.weight.data.copy_(embeds)
net.embedding.weight.requires_grad = False

lr, num_epochs = 0.01, 5
trainer = torch.optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction="none")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,devices)

def predict_sentiment(net, vocab, sequence):
  """预测⽂本序列的情感"""
  sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())
  label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)
  return 'positive' if label == 1 else 'negative'

predict_sentiment(net, vocab, 'this movie is so great')

predict_sentiment(net, vocab, 'this movie is so bad')

"""### bert微调实现语义推断"""

import os
import re
import torch
from torch import nn
from d2l import torch as d2l
d2l.DATA_HUB['SNLI'] = (
  'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
  '9fcde07509c7e87ec61c640c1b2753d9041758e4')
data_dir = d2l.download_extract('SNLI')

def read_snli(data_dir, is_train):
  """将SNLI数据集解析为前提、假设和标签"""
  def extract_text(s):
    # 删除我们不会使⽤的信息
    s = re.sub('\\(', '', s)
    s = re.sub('\\)', '', s)
    # ⽤⼀个空格替换两个或多个连续的空格
    s = re.sub('\\s{2,}', ' ', s)
    return s.strip()
  label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
  file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
  if is_train else 'snli_1.0_test.txt')
  with open(file_name, 'r') as f:
    rows = [row.split('\t') for row in f.readlines()[1:]]
  premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
  hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]
  labels = [label_set[row[0]] for row in rows if row[0] in label_set]
  return premises, hypotheses, labels

train_data = read_snli(data_dir, is_train=True)
for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):
  print('前提：', x0)
  print('假设：', x1)
  print('标签：', y)

import multiprocessing
class SNLIBERTDataset(torch.utils.data.Dataset):
  def __init__(self, dataset, max_len, vocab=None):
    all_premise_hypothesis_tokens = [[p_tokens, h_tokens] for p_tokens, h_tokens in zip(
                      *[d2l.tokenize([s.lower() for s in sentences])
                      for sentences in dataset[:2]])]
    self.labels = torch.tensor(dataset[2])
    self.vocab = vocab
    self.max_len = max_len
    (self.all_token_ids, self.all_segments,
    self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)
    print('read ' + str(len(self.all_token_ids)) + ' examples')
  def _preprocess(self, all_premise_hypothesis_tokens):
    pool = multiprocessing.Pool(4) # 使⽤4个进程
    out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)
    all_token_ids = [token_ids for token_ids, segments, valid_len in out]
    all_segments = [segments for token_ids, segments, valid_len in out]
    valid_lens = [valid_len for token_ids, segments, valid_len in out]
    return (torch.tensor(all_token_ids, dtype=torch.long),
          torch.tensor(all_segments, dtype=torch.long),
          torch.tensor(valid_lens))
  def _mp_worker(self, premise_hypothesis_tokens):
    p_tokens, h_tokens = premise_hypothesis_tokens
    self._truncate_pair_of_tokens(p_tokens, h_tokens)
    tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)
    token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \
            * (self.max_len - len(tokens))
    segments = segments + [0] * (self.max_len - len(segments))
    valid_len = len(tokens)
    return token_ids, segments, valid_len
  def _truncate_pair_of_tokens(self, p_tokens, h_tokens):
  # 为BERT输⼊中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置
    while len(p_tokens) + len(h_tokens) > self.max_len - 3:
      if len(p_tokens) > len(h_tokens):
        p_tokens.pop()
      else:
        h_tokens.pop()
  def __getitem__(self, idx):
    return (self.all_token_ids[idx], self.all_segments[idx],self.valid_lens[idx]), self.labels[idx]
  def __len__(self):
    return len(self.all_token_ids)

import json
d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',
'225d66f04cae318b841a13d32af3acc165f253ac')
d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',
'c72329e68a732bef0452e4b96a1c341c8910f81f')

def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout, max_len, devices):
  data_dir = d2l.download_extract(pretrained_model)
  # 定义空词表以加载预定义词表
  vocab = d2l.Vocab()
  vocab.idx_to_token = json.load(open(os.path.join(data_dir,'vocab.json')))
  vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}

  bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],
              ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,
              num_heads=4, num_layers=2, dropout=0.2,
              max_len=max_len, key_size=256, query_size=256,
              value_size=256, hid_in_features=256,
              mlm_in_features=256, nsp_in_features=256)
  
  # 加载预训练BERT参数
  bert.load_state_dict(torch.load(os.path.join(data_dir,'pretrained.params')))
  return bert, vocab

devices = d2l.try_all_gpus()
bert, vocab = load_pretrained_model('bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,
                  num_layers=2, dropout=0.1, max_len=512, devices=devices)

batch_size, max_len, num_workers = 512, 128, 2
data_dir = d2l.download_extract('SNLI')
train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)
test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)
train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,
                      num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(test_set, batch_size,
                      num_workers=num_workers)

class BERTClassifier(nn.Module):
  def __init__(self, bert):
    super(BERTClassifier, self).__init__()
    self.encoder = bert.encoder
    self.hidden = bert.hidden
    
    self.output = nn.Linear(256, 3)

  def forward(self, inputs):
    tokens_X, segments_X, valid_lens_x = inputs
    encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)
    return self.output(self.hidden(encoded_X[:, 0, :]))

net = BERTClassifier(bert)

lr, num_epochs = 1e-4, 5
trainer = torch.optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction='none')
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,devices)

